{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark Streaming - Exemplos Práticos\n",
    "\n",
    "Este notebook apresenta exemplos práticos de como utilizar o Spark Streaming com PySpark para o consumo de dados em tempo real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração Inicial da Aplicação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar SparkSession com configurações para streaming\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkStreamingExamples\") \\\n",
    "    .config(\"spark.sql.streaming.forceDeleteTempCheckpointLocation\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo 1\n",
    "## Consumindo dados em tempo-real a partir de arquivos CSV (File Source)\n",
    "\n",
    "Neste exemplo, o código faz a leitura de dados de uma pasta contendo arquivos CSV, filtra apenas os registros de acordo com uma condição definida pelo usuário. \n",
    "Em seguida, a saída pode ser impressa em tela ou gravada no HDFS ou em um banco de dados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o schema dos dados\n",
    "schema = StructType() \\\n",
    "    .add(\"id\", IntegerType()) \\\n",
    "    .add(\"nome\", StringType()) \\\n",
    "    .add(\"idade\", IntegerType())\n",
    "\n",
    "# Criar um dataframe a partir de uma pasta contendo arquivos csv\n",
    "df_stream = spark.readStream \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"pasta_entrada\")\n",
    "# -> A pasta \"pasta_entrada\" é uma pasta armazenada no HDFS e fica sendo a pasta de interesse, onde os dados a serem consumidos pela aplicação devem ser inseridos/armazenados\n",
    "\n",
    "# Aplicar alguma transformação sobre o dataframe de entrada (opcional)\n",
    "df_filtrado = df_stream.filter(\"idade >= 18\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saída do streaming (em console)\n",
    "query = df_filtrado.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"3 seconds\") \\\n",
    "    .option(\"checkpointlocation\", \"pasta_checkpoint\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n",
    "# -> O checkpointLocation permite que o sistema rastreie o progresso do processamento dos dados e possa retomar o processamento após uma falha.  Ele armazena metadados dos processamentos realizados em uma pasta, neste caso, no HDFS. \n",
    "# Evita que seja feito reprocessamento de dados anteriormente processados. \n",
    "\n",
    "# -> Neste exemplo de saída do Stream, os dados são apenas impressos em tela (\"console\"), não sendo salvos em nenhum lugar.\n",
    "\n",
    "# Aguardar a finalização do streaming\n",
    "query.awaitTermination()\n",
    "\n",
    "# Aguarda o encerramento manual da aplicação (Ctrl+C)\n",
    "#try:\n",
    "#    query.awaitTermination()\n",
    "#except KeyboardInterrupt:\n",
    "#    query.stop()\n",
    "#    print(\"Query parada com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encerrar o streaming (opcional)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saída do streaming (em csv no HDFS)\n",
    "query = df_filtrado.writeStream \\\n",
    "        .format(\"csv\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .trigger(processingTime=\"20 seconds\") \\\n",
    "        .option(\"checkpointlocation\", \"pasta_checkpoint\") \\\n",
    "        .option(\"path\", \"diretorio_saida\") \\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saída do streaming (em um banco de dados PostgreSQL)\n",
    "\n",
    "# Função para salvar os dados em um banco de dados PostgreSQL\n",
    "def salva_postgresql(df, epoch_id):\n",
    "    df.write.jdbc(\n",
    "        url=\"jdbc:postgresql://db:5432/mydb\",\n",
    "        table=\"tabela\",\n",
    "        mode=\"append\",\n",
    "        properties={\n",
    "            \"user\": \"postgres\",\n",
    "            \"password\": \"mypassword\",\n",
    "            \"driver\": \"org.postgresql.Driver\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Salvar o dataframe filtrado em um banco de dados PostgreSQL usando foreachBatch\n",
    "# -> O método foreachBatch permite que você execute uma função personalizada em cada microbatch de dados.\n",
    "# Neste caso, a função salva_postgresql é chamada para salvar os dados em um banco de dados PostgreSQL.\n",
    "# Mas podem ser aplicadas outras lógicas de código dentro dessa função definida, não necessariamente para salvar dados.\n",
    "query = df_filtrado.writeStream \\\n",
    "    .foreachBatch(salva_postgresql) \\\n",
    "    .option(\"checkpointLocation\", \"pasta_checkpoint\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "# Complementos aos comandos anteriores:\n",
    "\n",
    "# Existem três modos de saída disponíveis para o writeStream\n",
    "# Define-se .outputMode(parâmetro), onde o parâmetro pode ser:\n",
    "# - \"append\": apenas as novas linhas acrescentadas à tabela\n",
    "#             de resultados desde o último acionamento serão\n",
    "#             escritas no armazenamento externo. \n",
    "# - \"update\": apenas as linhas que foram atualizadas na tabela\n",
    "#             de resultados desde o último acionamento serão\n",
    "#             alteradas no armazenamento externo. \n",
    "# - \"complete\": a tabela de resultados atualizada inteira será\n",
    "#               escrita no armazenamento externo. \n",
    "#\n",
    "# Sintaxe:\n",
    "# query.writeStream.outputMode(\"MODO\")\n",
    "\n",
    "# É possível definir a frequência de acionamento do processamento do fluxo de dados usando o método trigger(). \n",
    "# Por padrão, o Spark Structured Streaming executa o processamento de fluxo de dados sempre que novos dados chegam à fonte de dados. \n",
    "# No entanto, é possível definir essa frequência de acionamento, como no exemplo a seguir: \n",
    "#\n",
    "# Sintaxe:\n",
    "# query.writeStream.trigger(processingTime='10 seconds')\n",
    "\n",
    "# Para parar um fuxo de processamento de dados\n",
    "# Sintaxe:\n",
    "#query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo 2\n",
    "## Consumindo streaming de dados em tempo-real a partir de sockets (Socket Streaming)\n",
    "\n",
    "Este exemplo conecta a um socket TCP e processa dados de texto em tempo real. Este servidor socket simula a recepção de dados em tempo real, enviando mensagens a cada 3 segundos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas necessárias para o servidor socket\n",
    "import time\n",
    "import threading\n",
    "import socket\n",
    "import random\n",
    "\n",
    "# Função para criar um servidor socket simples\n",
    "def socket_server():\n",
    "    server = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    server.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n",
    "    server.bind(('localhost', 9999))\n",
    "    server.listen(1)\n",
    "    \n",
    "    print(\"Servidor socket iniciado na porta 9999\")\n",
    "    \n",
    "    try:\n",
    "        conn, addr = server.accept()\n",
    "        print(f\"Conexão aceita de {addr}\")\n",
    "        \n",
    "        # Enviar dados de exemplo\n",
    "        messages = [\n",
    "            \"ola mundo spark streaming\",\n",
    "            \"apache spark eh legal\",\n",
    "            \"streaming em tempo real\",\n",
    "            \"tempo-real dados\",\n",
    "            \"dados spark streaming\"\n",
    "        ]\n",
    "        \n",
    "        for msg in messages:\n",
    "            conn.send(f\"{msg}\\n\".encode())\n",
    "            time.sleep(5)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Erro no servidor: {e}\")\n",
    "    finally:\n",
    "        server.close()\n",
    "\n",
    "# Iniciar servidor em thread separada\n",
    "server_thread = threading.Thread(target=socket_server)\n",
    "server_thread.daemon = True\n",
    "server_thread.start()\n",
    "\n",
    "# Aguardar servidor iniciar\n",
    "time.sleep(2)\n",
    "print(\"Servidor socket em execução...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar streaming DataFrame que lê do socket\n",
    "socket_stream = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# Processar dados: dividir linhas em palavras e contar\n",
    "words = socket_stream.select(\n",
    "    explode(split(socket_stream.value, \" \")).alias(\"word\")\n",
    ")\n",
    "\n",
    "word_counts = words.groupBy(\"word\").count().orderBy(\"count\", ascending=False)\n",
    "\n",
    "print(\"Stream de socket configurado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar query de streaming\n",
    "socket_query = word_counts.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .trigger(processingTime=\"5 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Query de socket streaming iniciada! Aguarde...\")\n",
    "\n",
    "# Aguardar para ver os resultados\n",
    "time.sleep(20)\n",
    "\n",
    "# Parar a query\n",
    "#socket_query.stop()\n",
    "#print(\"Query de socket parada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo 3\n",
    "## Geração de fluxos de dados (e consumo) baseado em um Rate Source (recomendado para testes)\n",
    "\n",
    "O rate source gera registros em uma taxa constante e configurável. Ideal para testes de desempenho.\n",
    "\n",
    "Cada registro geralmente contém:\n",
    "- timestamp: O momento em que o registro foi gerado.\n",
    "- value: Um número sequencial, que aumenta a cada novo registro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar streaming DataFrame com rate source\n",
    "rate_stream = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 2) \\\n",
    "    .option(\"rampUpTime\", \"5s\") \\\n",
    "    .load()\n",
    "\n",
    "# Adicionar colunas computadas\n",
    "enriched_stream = rate_stream.select(\n",
    "    \"timestamp\",\n",
    "    \"value\",\n",
    "    (col(\"value\") % 3).alias(\"categoria\"),\n",
    "    (col(\"value\") * 10).alias(\"resultado\"),\n",
    "    when(col(\"value\") % 2 == 0, \"par\").otherwise(\"impar\").alias(\"paridade\")\n",
    ")\n",
    "\n",
    "print(\"Rate stream configurado!\")\n",
    "enriched_stream.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregar dados por categoria\n",
    "aggregated = enriched_stream \\\n",
    "    .groupBy(\"categoria\", \"paridade\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"contador\"),\n",
    "        avg(\"resultado\").alias(\"media\"),\n",
    "        max(\"value\").alias(\"maximoe\")\n",
    "    )\n",
    "\n",
    "# Executar streaming query\n",
    "rate_query = aggregated.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .trigger(processingTime=\"3 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Rate streaming iniciado! Aguarde...\")\n",
    "\n",
    "# Aguardar para ver os resultados\n",
    "time.sleep(15)\n",
    "\n",
    "# Parar a query\n",
    "rate_query.stop()\n",
    "print(\"Rate streaming parado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplo 4\n",
    "## Watermarks (Marca d'água) para lidar com dados atrasados\n",
    "\n",
    "Suponha que estamos processando eventos que enviam dados a cada minuto, e queremos que os resultados de processamento estejam sempre atualizados com no máximo um atraso de 5 minutos em relação aos eventos mais recentes. Para isso, podemos definir uma janela (watermark) de 5 minutos usando o withWatermark(). \n",
    "\n",
    "Isso significa que, se um evento chegar com um timestamp que está mais de 5 minutos atrasado em relação ao watermark, ele será descartado, já que não é considerado como atualizado. \n",
    "\n",
    "Quando um dado é recebido pelo Spark Streaming, ele é carimbado com um timestamp que indica quando esse dado foi gerado. Com o withWatermark(), é possível especificar uma janela de tempo a partir desse timestamp, dentro da qual os dados são considerados atualizados e são processados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o schema dos dados de entrada, com os tipos esperados para cada coluna\n",
    "meuschema = StructType([\n",
    "    StructField(\"ID_TRANSACAO\", StringType(), True),\n",
    "    StructField(\"ID_USUARIO\", StringType(), True),\n",
    "    StructField(\"DATA_HORA_EVENTO\", TimestampType(), True),\n",
    "    StructField(\"TIPO_EVENTO\", StringType(), True),\n",
    "    StructField(\"VALOR_TRANSACAO\", DoubleType(), True),\n",
    "    StructField(\"MOEDA\", StringType(), True),\n",
    "    StructField(\"ITEM_ID\", StringType(), True),\n",
    "    StructField(\"QUANTIDADE\", IntegerType(), True),\n",
    "    StructField(\"LOCALIZACAO_IP\", StringType(), True),\n",
    "    StructField(\"STATUS_EVENTO\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Cria um DataFrame de streaming lendo arquivos CSV de uma pasta\n",
    "# O Spark vai processar 1 arquivo por vez a cada trigger\n",
    "dfEntrada = spark.readStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .schema(meuschema) \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \",\") \\\n",
    "    .option(\"timestampFormat\", \"yyyy-MM-dd HH:mm:ss\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(\"pasta_csv\")\n",
    "\n",
    "# Aplica watermark para considerar dados com atraso de até 1 hora, usando a coluna de tempo do evento\n",
    "# Em seguida, agrupa os eventos em janelas de 2 minutos e conta quantos eventos de cada tipo ocorreram\n",
    "# O resultado inclui início e fim da janela, tipo do evento e total de eventos\n",
    "dfEventTime = dfEntrada \\\n",
    "    .withWatermark(\"DATA_HORA_EVENTO\", \"1 hours\") \\\n",
    "    .groupBy(\n",
    "        window(\"DATA_HORA_EVENTO\", \"2 minutes\"),\n",
    "        \"TIPO_EVENTO\"\n",
    "    ) \\\n",
    "    .count() \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"janela_inicio\"),\n",
    "        col(\"window.end\").alias(\"janela_fim\"),\n",
    "        col(\"TIPO_EVENTO\"),\n",
    "        col(\"count\").alias(\"total_eventos\")\n",
    "    ).orderBy(\"janela_inicio\", \"TIPO_EVENTO\")\n",
    "\n",
    "# Define a saída do stream para o console\n",
    "# O modo \"complete\" imprime o resultado completo a cada trigger\n",
    "# A trigger está configurada para rodar a cada 20 segundos\n",
    "# O checkpoint é usado para manter o estado entre execuções (falhas/reinícios)\n",
    "query = dfEventTime.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"numRows\", 100) \\\n",
    "    .trigger(processingTime='20 seconds') \\\n",
    "    .option(\"checkpointLocation\", \"pasta_checkpoint\") \\\n",
    "    .start()\n",
    "\n",
    "# Aguarda o encerramento manual da aplicação (Ctrl+C)\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    query.stop()\n",
    "    print(\"Query parada com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpeza e Encerramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parar todas as queries ativas\n",
    "for query in spark.streams.active:\n",
    "    print(f\"Parando query: {query.name or query.id}\")\n",
    "    query.stop()\n",
    "\n",
    "print(\"Todas as queries foram paradas.\")\n",
    "\n",
    "# Parar SparkSession\n",
    "spark.stop()\n",
    "print(\"SparkSession encerrada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursos Adicionais\n",
    "\n",
    "- [Documentação Oficial Spark Streaming](https://spark.apache.org/docs/latest/streaming/index.html)\n",
    "- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)\n",
    "- [Spark Streaming + Kafka Integration](https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
