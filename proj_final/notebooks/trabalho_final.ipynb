{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5a4c3b-cb2f-44af-a638-7f3a05b3963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar as bibliotecas\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6615ac0-0b0d-44b9-b836-9f5cb18391a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/myuser/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/myuser/.ivy2/cache\n",
      "The jars for the packages stored in: /home/myuser/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e1e0a611-30d5-4863-afab-95c90397cf33;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.4.0 in central\n",
      "mongodb#mongodb-driver-sync;5.1.4 in central_2.12;3.4.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.4.0 in central\n",
      "\n",
      "\t[5.1.4] org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99)\n",
      "\tfound org.mongodb#bson;5.1.4 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;5.1.4 in central\n",
      "\tfound org.mongodb#bson-record-codec;5.1.4 in central\n",
      ":: resolution report :: resolve 3301ms :: artifacts dl 22ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.mongodb#bson;5.1.4 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;5.1.4 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.4.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   1   |   0   |   0   ||   16  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e1e0a611-30d5-4863-afab-95c90397cf33\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 16 already retrieved (0kB/12ms)\n",
      "Setting default log level to \"WARN\". central_2.12;3.4.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.3.2 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.4.0 in central\n",
      "\n",
      "\t[5.1.4] org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99)\n",
      "\tfound org.mongodb#bson;5.1.4 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;5.1.4 in central\n",
      "\tfound org.mongodb#bson-record-codec;5.1.4 in central\n",
      ":: resolution report :: resolve 3301ms :: artifacts dl 22ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.mongodb#bson;5.1.4 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;5.1.4 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.4.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   1   |   0   |   0   ||   16  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e1e0a611-30d5-4863-afab-95c90397cf33\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 16 already retrieved (0kB/12ms)\n",
      "\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/21 01:13:54 WARN Client: Same path resource file:///home/myuser/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.4.1.jar added multiple times to distributed cache.rial.snappy#snappy-java;1.1.10.1 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.6 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.4.0 in central\n",
      "\n",
      "\t[5.1.4] org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99)\n",
      "\tfound org.mongodb#bson;5.1.4 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;5.1.4 in central\n",
      "\tfound org.mongodb#bson-record-codec;5.1.4 in central\n",
      ":: resolution report :: resolve 3301ms :: artifacts dl 22ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.4.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.mongodb#bson;5.1.4 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;5.1.4 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.4.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.6 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   1   |   0   |   0   ||   16  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e1e0a611-30d5-4863-afab-95c90397cf33\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 16 already retrieved (0kB/12ms)\n",
      "\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "\n",
      "25/07/21 01:13:54 WARN Client: Same path resource file:///home/myuser/.ivy2/jars/org.mongodb.spark_mongo-spark-connector_2.12-10.4.0.jar added multiple times to distributed cache.\n",
      "25/07/21 01:13:54 WARN Client: Same path resource file:///home/myuser/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.4.1.jar added multiple times to distributed cache.\n",
      "25/07/21 01:13:54 WARN Client: Same path resource file:///home/myuser/.ivy2/jars/org.apache.kafka_kafka-clients-3.3.2.jar added multiple times to distributed cache.\n",
      "25/07/21 01:13:54 WARN Client: Same path resource file:///home/myuser/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar added multiple times to distributed cache.\n",
      "25/07/21 01:13:54 WARN Client: Same path resource file:///home/myuser/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar added multiple times to distributed cache.\n",
      "25/07/21 01:13:54 WARN Client: Same path resource file:///home/myuser/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar added multiple times to distributed cache.\n",
      "25/07/21 01:13:54 WARN Client: Same path resource file:///home/myuser/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar added multiple times to distributed cache.\n",
      "25/07/21 01:13:54 WARN Client: Same path resource file:///home/myuser/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.1.jar added multiple times to distributed cache.\n",
      "25/07/21 01:13:54 WARN Client: Same path resource file:///home/myuser/.ivy2/jars/org.slf4j_slf4j-api-2.0.6.jar added multiple times to distributed cache.\n",
      "25/07/21 01:13:54 WARN Client: Same path resource file:///home/myuser/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar added multiple times to distributed cache.\n",
      "25/07/21 01:13:54 WARN Client: Same path resource file:///home/myuser/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar added multiple times to distributed cache.\n",
      "25/07/21 01:13:54 WARN Client: Same path resource file:///home/myuser/.ivy2/jars/org.mongodb_mongodb-driver-sync-5.1.4.jar added multiple times to distributed cache.\n",
      "25/07/21 01:13:54 WARN Client: Same path resource file:///home/myuser/.ivy2/jars/org.mongodb_bson-5.1.4.jar added multiple times to distributed cache.\n",
      "25/07/21 01:13:54 WARN Client: Same path resource file:///home/myuser/.ivy2/jars/org.mongodb_mongodb-driver-core-5.1.4.jar added multiple times to distributed cache.\n",
      "25/07/21 01:13:54 WARN Client: Same path resource file:///home/myuser/.ivy2/jars/org.mongodb_bson-record-codec-5.1.4.jar added multiple times to distributed cache.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"trabalho-final\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,org.mongodb.spark:mongo-spark-connector_2.12:10.4.0\") \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \"mongodb://spark-mongo:27017/spark-db.music\") \\\n",
    "    .config(\"spark.mongodb.write.connection.uri\", \"mongodb://spark-mongo:27017/spark-db.music\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e88fe53-e9e0-453c-abb4-00583c788531",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Colocando dados (.json) no MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35eed4ba-0e24-45b2-9f2f-a7f2ae4b3d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|      asin| helpful|overall|          reviewText| reviewTime|    reviewerID|        reviewerName|             summary|unixReviewTime|\n",
      "+----------+--------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "|1384719342|  [0, 0]|    5.0|Not much to write...|02 28, 2014|A2IBPI20UZIR0U|cassandra tu \"Yea...|                good|    1393545600|\n",
      "|1384719342|[13, 14]|    5.0|The product does ...|03 16, 2013|A14VAT5EAX3D9S|                Jake|                Jake|    1363392000|\n",
      "|1384719342|  [1, 1]|    5.0|The primary job o...|08 28, 2013|A195EZSQDW3E21|Rick Bennette \"Ri...|It Does The Job Well|    1377648000|\n",
      "|1384719342|  [0, 0]|    5.0|Nice windscreen p...|02 14, 2014|A2C00NNG1ZQQG2|RustyBill \"Sunday...|GOOD WINDSCREEN F...|    1392336000|\n",
      "|1384719342|  [0, 0]|    5.0|This pop filter i...|02 21, 2014| A94QU4C90B1AX|       SEAN MASLANKA|No more pops when...|    1392940800|\n",
      "|B00004Y2UT|  [0, 0]|    5.0|So good that I bo...|12 21, 2012|A2A039TZMZHH9Y| Bill Lewey \"blewey\"|      The Best Cable|    1356048000|\n",
      "|B00004Y2UT|  [0, 0]|    5.0|I have used monst...|01 19, 2014|A1UPZM995ZAH90|               Brian|Monster Standard ...|    1390089600|\n",
      "|B00004Y2UT|  [0, 0]|    3.0|I now use this ca...|11 16, 2012| AJNFQI3YR6XJ5|   Fender Guy \"Rick\"|Didn't fit my 199...|    1353024000|\n",
      "|B00004Y2UT|  [0, 0]|    5.0|Perfect for my Ep...| 07 6, 2008|A3M1PLEYNDEYO8|     G. Thomas \"Tom\"|         Great cable|    1215302400|\n",
      "|B00004Y2UT|  [0, 0]|    5.0|Monster makes the...| 01 8, 2014| AMNTZU1YQN1TH|         Kurt Robair|Best Instrument C...|    1389139200|\n",
      "|B00004Y2UT|  [6, 6]|    5.0|Monster makes a w...|04 19, 2012|A2NYK9KWFMJV4Y|Mike Tarrani \"Jaz...|One of the best i...|    1334793600|\n",
      "|B00005ML71|  [0, 0]|    4.0|I got it to have ...|04 22, 2014|A35QFQI0M46LWO|       Christopher C|It works great bu...|    1398124800|\n",
      "|B00005ML71|  [0, 0]|    3.0|If you are not us...|11 17, 2013|A2NIT6BKW11XJQ|                 Jai|HAS TO GET USE TO...|    1384646400|\n",
      "|B00005ML71|  [0, 0]|    5.0|I love it, I used...|06 16, 2013|A1C0O09LOLVI39|             Michael|             awesome|    1371340800|\n",
      "|B00005ML71|  [0, 0]|    5.0|I bought this to ...|12 31, 2012|A17SLR18TUMULM|         Straydogger|           It works!|    1356912000|\n",
      "|B00005ML71|  [0, 0]|    2.0|I bought this to ...|08 17, 2013|A2PD27UKAD3Q00|Wilhelmina Zeitge...|Definitely Not Fo...|    1376697600|\n",
      "|B000068NSX|  [0, 0]|    4.0|This Fender cable...|08 13, 2013| AKSFZ4G1AXYFC|        C.E. \"Frank\"|Durable Instrumen...|    1376352000|\n",
      "|B000068NSX|  [0, 0]|    5.0|wanted it just on...| 07 9, 2013| A67OJZLHBBUQ9|Charles F. Marks ...|fender 18 ft. Cal...|    1373328000|\n",
      "|B000068NSX|  [3, 3]|    5.0|I've been using t...|03 18, 2013|A2EZWZ8MBEDOLN|              Charlo|So far so good.  ...|    1363564800|\n",
      "|B000068NSX|  [0, 0]|    5.0|Fender cords look...| 08 7, 2013|A1CL807EOUPVP1|             GunHawk|Add California to...|    1375833600|\n",
      "+----------+--------+-------+--------------------+-----------+--------------+--------------------+--------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lendo o arquivo .json do HDFS\n",
    "\n",
    "caminho_json = \"hdfs://spark-master:9000/user/myuser/Musical_Instruments_5.json\"\n",
    "\n",
    "df = spark.read.json(caminho_json)\n",
    "\n",
    "# Mostrar os dados\n",
    "df.show()  # truncate=False evita cortar strings longas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba416a3c-1013-4267-b051-73ff64c34579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ====================>                             (1 + 1) / 2]"
     ]
    }
   ],
   "source": [
    "df.write \\\n",
    "  .format(\"mongodb\") \\\n",
    "  .option(\"spark.mongodb.write.connection.uri\", \"mongodb://spark-mongo:27017/spark-db.music\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff80ee2-2484-40d4-8883-5d910206bcf3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Colocando Dados no Postgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6917ffbe-a5b1-4318-8f52-cb83d8c6dde6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+----------+---------+--------------------+--------------------+--------------------+------------+----------+-------+-------+\n",
      "|   spotify_track_uri|                 ts|  platform|ms_played|          track_name|         artist_name|          album_name|reason_start|reason_end|shuffle|skipped|\n",
      "+--------------------+-------------------+----------+---------+--------------------+--------------------+--------------------+------------+----------+-------+-------+\n",
      "|2J3n32GeLmMjwuAzy...|2013-07-08 02:44:34|web player|     3185| Say It, Just Say It|        The Mowgli's|Waiting For The Dawn|    autoplay|  clickrow|  FALSE|  FALSE|\n",
      "|1oHxIPqJyvAYHy0PV...|2013-07-08 02:45:37|web player|    61865|Drinking from the...|       Calvin Harris|           18 Months|    clickrow|  clickrow|  FALSE|  FALSE|\n",
      "|487OPlneJNni3NWC8...|2013-07-08 02:50:24|web player|   285386|         Born To Die|        Lana Del Rey|Born To Die - The...|    clickrow|   unknown|  FALSE|  FALSE|\n",
      "|5IyblF777jLZj1vGH...|2013-07-08 02:52:40|web player|   134022|    Off To The Races|        Lana Del Rey|Born To Die - The...|   trackdone|  clickrow|  FALSE|  FALSE|\n",
      "|0GgAAB0ZMllFhbNc3...|2013-07-08 03:17:52|web player|        0|           Half Mast|   Empire Of The Sun|  Walking On A Dream|    clickrow|   nextbtn|  FALSE|  FALSE|\n",
      "|50VNvhzyaSplJCKWc...|2013-07-08 03:17:52|web player|    63485|          Impossible|        James Arthur|          Impossible|    clickrow|  clickrow|  FALSE|  FALSE|\n",
      "|1I4EczxGBcPR3J3Ke...|2013-07-08 03:17:56|web player|        0|      We Own The Sky|                 M83|   Saturdays = Youth|     nextbtn|   nextbtn|  FALSE|  FALSE|\n",
      "|5arVt2Wg0zbiWwAOZ...|2013-07-08 03:17:56|web player|     1268|Higher Ground - R...|Red Hot Chili Pep...|       Mother's Milk|     nextbtn|   nextbtn|  FALSE|  FALSE|\n",
      "|1ixtaZc0Adil3yD1I...|2013-07-08 03:17:58|web player|        0|       Happy Up Here|            Röyksopp|       Happy Up Here|     nextbtn|   nextbtn|  FALSE|  FALSE|\n",
      "|2v5mpowLQNFN7NC46...|2013-07-08 03:19:11|web player|        0|             Phantom|             Justice|             Phantom|     nextbtn|  clickrow|  FALSE|  FALSE|\n",
      "|07hII2Rc29q4F2nTE...|2013-07-08 03:20:20|web player|    67587|Sun - Gildas Kits...|Two Door Cinema Club|The Kitsuné Speci...|    clickrow|  clickrow|  FALSE|  FALSE|\n",
      "|4kO7mrAPfqIrsKwUO...|2013-07-08 03:20:36|web player|    12846|       Midnight City|                 M83|Hurry Up, We're D...|    clickrow|  clickrow|  FALSE|  FALSE|\n",
      "|4oTIuUmpE2xdXrpon...|2013-07-08 03:21:13|web player|    36132|              Heaven|         Emeli Sandé|Our Version Of Ev...|    clickrow|  clickrow|  FALSE|  FALSE|\n",
      "|49h0RYK3yzWkfbVyN...|2013-07-08 03:22:51|web player|    95817|    Do I Wanna Know?|      Arctic Monkeys|    Do I Wanna Know?|    clickrow|  clickrow|  FALSE|  FALSE|\n",
      "|4iG2gAwKXsOcijVaV...|2013-07-08 03:22:54|web player|     1763|     Time to Pretend|                MGMT|Oracular Spectacular|    clickrow|   nextbtn|  FALSE|  FALSE|\n",
      "|19K3lUMJmOdeuOBTr...|2013-07-08 03:33:38|web player|    45712|        Weekend Wars|                MGMT|Oracular Spectacular|     nextbtn|   nextbtn|  FALSE|  FALSE|\n",
      "|5nv854ey1k43KaZ0k...|2013-07-08 03:37:30|web player|   228021|           The Youth|                MGMT|Oracular Spectacular|     nextbtn|   unknown|  FALSE|  FALSE|\n",
      "|3FtYbEfBqAlGO46NU...|2013-07-08 03:41:21|web player|   229589|       Electric Feel|                MGMT|Oracular Spectacular|   trackdone| trackdone|  FALSE|  FALSE|\n",
      "|1jJci4qxiYcOHhQR2...|2013-07-08 03:41:30|web player|     7332|                Kids|                MGMT|Oracular Spectacular|   trackdone|  clickrow|  FALSE|  FALSE|\n",
      "|4Sfa7hdVkqlM8UW5L...|2013-07-08 03:45:10|web player|   217935|         Take a Walk|         Passion Pit|            Gossamer|    clickrow|  clickrow|  FALSE|  FALSE|\n",
      "+--------------------+-------------------+----------+---------+--------------------+--------------------+--------------------+------------+----------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lendo o arquivo .csv do HDFS\n",
    "\n",
    "caminho_csv = \"hdfs://spark-master:9000/user/myuser/spotify_history.csv\"\n",
    "\n",
    "# Ler o arquivo CSV do HDFS\n",
    "df = spark.read.csv(caminho_csv, header=True, inferSchema=True)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4acfd325-0e17-4cd8-8515-1cf6b144be35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ====================>                             (1 + 1) / 2]"
     ]
    }
   ],
   "source": [
    "# Gravar dados no banco postgresql\n",
    "df.write.format(\"jdbc\") \\\n",
    "\t.option(\"url\",\"jdbc:postgresql://spark-postgres:5432/spark-db\") \\\n",
    "\t.option(\"dbtable\",\"music\") \\\n",
    "\t.option(\"user\",\"postgres\") \\\n",
    "\t.option(\"password\",\"postgres123\") \\\n",
    "\t.option(\"driver\",\"org.postgresql.Driver\") \\\n",
    "\t.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7667e11a-571b-4b5b-93ea-ffc34315dbfa",
   "metadata": {},
   "source": [
    "# Processando os dados em tempo real consumindo o topico (MongoDB) do Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8fcdcc0-399e-4efc-8789-fe04ea858500",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Criar o dataframe do tipo stream, apontando para o servidor kafka e o tópico a ser consumido.\n",
    "df = (spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \"spark-master:9092\")\n",
    "        .option(\"subscribe\", \"topico-mongo.spark-db.music\")\n",
    "        .option(\"startingOffsets\", \"earliest\") \n",
    "        .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e29b997-f896-4f53-ae01-46a652489e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"payload\", StructType([\n",
    "        StructField(\"after\", StringType(), True)  # <<< importante: after é uma string!\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# Schema interno que está codificado dentro da string JSON do campo \"after\"\n",
    "after_schema = StructType([\n",
    "    StructField(\"_id\", StructType([\n",
    "        StructField(\"$oid\", StringType(), True)\n",
    "    ])),\n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"helpful\", ArrayType(\n",
    "        StructType([StructField(\"$numberLong\", StringType(), True)])\n",
    "    )),\n",
    "    StructField(\"overall\", DoubleType(), True),\n",
    "    StructField(\"reviewText\", StringType(), True),\n",
    "    StructField(\"reviewTime\", StringType(), True),\n",
    "    StructField(\"reviewerID\", StringType(), True),\n",
    "    StructField(\"reviewerName\", StringType(), True),\n",
    "    StructField(\"summary\", StringType(), True),\n",
    "    StructField(\"unixReviewTime\", StructType([\n",
    "        StructField(\"$numberLong\", StringType(), True)\n",
    "    ]))\n",
    "])\n",
    "\n",
    "# 1. Primeiro: desserializa o valor inteiro do Kafka (JSON principal)\n",
    "df1 = df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"))\n",
    "\n",
    "# 2. Segundo: desserializa o campo 'after' (string JSON) com o schema interno\n",
    "df2 = df1.select(from_json(col(\"data.payload.after\"), after_schema).alias(\"after\"))\n",
    "\n",
    "dx = df2.select(\"after.asin\", \"after.helpful\", \"after.overall\", \"after.reviewTime\", \"after.reviewerName\", \"after.summary\")\n",
    "\n",
    "#pegando as compras com o overall maior que 4\n",
    "df_maior_overall = dx.filter(dx.overall > 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005911fd-2ff2-4379-b6f6-d878672cdaa6",
   "metadata": {},
   "source": [
    "# Escrevendo os dados processados no MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9ef89c8-d47c-4c3c-b47a-c362b5b4a0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 01:14:42 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f1b712ca270>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 01:14:43 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    }
   ],
   "source": [
    "df_maior_overall.writeStream \\\n",
    "    .format(\"mongodb\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/mongo\") \\\n",
    "    .option(\"database\", \"spark-db\") \\\n",
    "    .option(\"collection\", \"best-products\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538eb3ce-d20d-475c-bc57-0f2688204578",
   "metadata": {},
   "source": [
    "# Processando os dados em tempo real consumindo o topico (Postgres) do Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68747506-90e9-4c9c-bd99-3ae999459d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar o dataframe do tipo stream, apontando para o servidor kafka e o tópico a ser consumido.\n",
    "df_postgress = (spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \"spark-master:9092\")\n",
    "        .option(\"subscribe\", \"music.public.music\")\n",
    "        .option(\"startingOffsets\", \"earliest\") \n",
    "        .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8db5780-3510-4975-be88-fbabad72d225",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valores = df_postgress.selectExpr(\"CAST(value AS STRING)\")\n",
    "query = (\n",
    "    df_valores.writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"console\")\n",
    "    .option(\"truncate\", False)\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00b22c65-952c-4c63-b206-9ea483c86ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 01:20:29 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9b7775a0-ec4d-4e1e-bc7a-3be6d5598947. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp c                                                                                r, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "1) / 1]r, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "[Stage 5:>                                                          (0 + 0) / 1]r, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "[Stage 6:==============>                                           (4 + 2) / 16]r, value.deserializer, enable.auto.commit, max.poll.records, auto.1) / 1]r, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "[Stage 5:>                                                          (0 + 0) / 1]r, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "1) / 1]r, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "[Stage 5:>                                                          (0 + 0) / 1]r, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "[Stage 6:===================================>                     (10 + 2) / 16]up[Stage 6:==============>                                           (4 + 2) / 16]r, value.deserializer, enable.auto.commit, max.poll.records, auto.1) / 1]r, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "[Stage 5:>                                                          (0 + 0) / 1]r, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "1) / 1]r, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "[Stage 5:>                                                          (0 + 0) / 1]r, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "[Stage 6:==============>                                           (4 + 2) / 16]r, value.deserializer, enable.auto.commit, max.poll.records, auto.1) / 1]r, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "[Stage 5:>                                                          (0 + 0) / 1]r, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "1) / 1]r, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "[Stage 5:>                                                          (0 + 0) / 1]r, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----------------+---------------+\n",
      "|spotify_track_uri|total_execucoes|\n",
      "+-----------------+---------------+\n",
      "|NULL             |10261          |\n",
      "+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"payload\", StructType([\n",
    "        StructField(\"after\", StringType(), True)  # <<< importante: after é uma string!\n",
    "    ]))\n",
    "])\n",
    "\n",
    "spotify_schema = StructType([\n",
    "    StructField(\"spotify_track_uri\", StringType(), False),\n",
    "    StructField(\"ts\", TimestampType(), False),\n",
    "    StructField(\"platform\", StringType(), False),\n",
    "    StructField(\"ms_played\", IntegerType(), False),\n",
    "    StructField(\"track_name\", StringType(), False),\n",
    "    StructField(\"artist_name\", StringType(), False),\n",
    "    StructField(\"album_name\", StringType(), False),\n",
    "    StructField(\"reason_start\", StringType(), True),\n",
    "    StructField(\"reason_end\", StringType(), True),\n",
    "    StructField(\"shuffle\", BooleanType(), True),\n",
    "    StructField(\"skipped\", BooleanType(), True)\n",
    "])\n",
    "\n",
    "# 1. Primeiro: desserializa o valor inteiro do Kafka (JSON principal)\n",
    "df1 = df.select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\"))\n",
    "\n",
    "# 2. Segundo: desserializa o campo 'after' (string JSON) com o schema interno\n",
    "df2 = df1.select(from_json(col(\"data.payload.after\"), spotify_schema).alias(\"after\"))\n",
    "\n",
    "# Use watermark para permitir agregações com stateful streaming\n",
    "\n",
    "dx = df2.select(\n",
    "    to_timestamp(col(\"after.ts\")).alias(\"ts\"),\n",
    "    col(\"after.platform\").alias(\"platform\")\n",
    ").withWatermark(\"ts\", \"10 minutes\")\n",
    "\n",
    "df_agg_plataforma = dx.groupBy(\"platform\").agg(count(\"*\").alias(\"total_execucoes\"))\n",
    "\n",
    "ds = (\n",
    "    df_agg_musicas.writeStream\n",
    "    .outputMode(\"update\")  # <---- aqui muda\n",
    "    .format(\"console\")\n",
    "    .option(\"truncate\", False)\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0003c00-6ac7-473a-b1d9-9233d825721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_postgress.selectExpr(\"CAST(value AS STRING)\").writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36dde89-61f3-43ec-bbff-ed201f8b1a73",
   "metadata": {},
   "source": [
    "# Escrevendo os dados processados no Postgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61c79d74-ef0e-464b-a960-3a91daf227f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 01:27:14 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f1ba081e0c0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/21 01:27:14 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "25/07/21 01:27:16 ERROR MicroBatchExecution: Query [id = 3a7db58f-8903-4aca-a622-139e657705d7, runId = 1cc88b14-00fb-4a0d-be53-c1fb4bf377ec] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pyspark/sql/utils.py\", line 120, in call\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pyspark/sql/utils.py\", line 117, in call\n",
      "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
      "  File \"/tmp/ipykernel_8942/1030542473.py\", line 10, in write_to_postgres\n",
      "    .save()\n",
      "     ^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pyspark/sql/readwriter.py\", line 1461, in save\n",
      "    self._jwrite.save()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\", line 185, in deco\n",
      "    raise converted from None\n",
      "pyspark.errors.exceptions.captured.AnalysisException: Column total_execucoes not found in schema Some(StructType(StructField(spotify_track_uri,StringType,true),StructField(ts,TimestampType,true),StructField(platform,StringType,true),StructField(ms_played,IntegerType,true),StructField(track_name,StringType,true),StructField(artist_name,StringType,true),StructField(album_name,StringType,true),StructField(reason_start,StringType,true),StructField(reason_end,StringType,true),StructField(shuffle,StringType,true),StructField(skipped,StringType,true))).\n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat com.sun.proxy.$Proxy53.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n"
     ]
    }
   ],
   "source": [
    "def write_to_postgres(batch_df, batch_id):\n",
    "    (batch_df.write\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"url\", \"jdbc:postgresql://spark-postgres:5432/spark-db\")\n",
    "        .option(\"dbtable\", \"music\")\n",
    "        .option(\"user\", \"postgres\")\n",
    "        .option(\"password\", \"postgres123\")\n",
    "        .option(\"driver\", \"org.postgresql.Driver\")\n",
    "        .mode(\"append\")\n",
    "        .save()\n",
    "    )\n",
    "\n",
    "# Stream que escreve no Postgres\n",
    "df_agg_plataforma.writeStream \\\n",
    "    .foreachBatch(write_to_postgres) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/checkpoints/execucoes_plataforma\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f156f65-11b9-4a34-bbf7-fd4bfa3cbc24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
